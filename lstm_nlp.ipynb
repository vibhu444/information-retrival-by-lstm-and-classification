{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Loading of the library require for the loading dataset and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  dataset = ( 18 books in txt format)\n",
    " importing   library for preprocesssing and loading data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os \n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "from sklearn.model_selection  import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #  loading data from path \"/books\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loading(path):\n",
    "    inputfile = os.path.join(path)\n",
    "    with open (inputfile) as f:\n",
    "        book = f.read()\n",
    "        return book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './books/'\n",
    "bookfiles = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "bookfiles = bookfiles[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = []\n",
    "for  i in bookfiles:\n",
    "    books.append(loading( path+ i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\\\rtf1\\\\ansi\\\\ansicpg1252\\\\cocoartf1404\\\\cocoasubrtf470\\n{\\\\fonttbl\\\\f0\\\\fmodern\\\\fcharset0 Courier;}\\n{\\\\color'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books[0][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# removing the other chracter from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'\\n', ' ', text)  # removing the /n from the above text \n",
    "    text = re.sub(r'[{}@_*>()\\\\#%+=\\[\\]]','', text) # removing the above seen \\\\  from the text \n",
    "    text = re.sub('a0','', text)\n",
    "    text = re.sub('\\'92t','\\'t', text)\n",
    "    text = re.sub('\\'92s','\\'s', text)\n",
    "    text = re.sub('\\'92m','\\'m', text)\n",
    "    text = re.sub('\\'92ll','\\'ll', text)\n",
    "    text = re.sub('\\'91','', text)\n",
    "    text = re.sub('\\'92','', text)\n",
    "    text = re.sub('\\'93','', text)\n",
    "    text = re.sub('\\'94','', text)\n",
    "    text = re.sub('\\.','. ', text)\n",
    "    text = re.sub('\\!','! ', text)\n",
    "    text = re.sub('\\?','? ', text)\n",
    "    text = re.sub(' +',' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# append all the clean words in book_clean [] list  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_clean = []\n",
    "for i in books:\n",
    "    book_clean.append(clean_text(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rtf1ansiansicpg1252cocoartf1404cocoasubrtf470 fonttblf0fmodernfcharset0 Courier; colortbl;red255gree'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now see the clean text data \n",
    "book_clean[0][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making dictionary (vocab = {} ) which contain all the character with there respective occurance value in txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "count = 0 \n",
    "for i in book_clean:\n",
    "    for char in i :\n",
    "        if char not in vocab:\n",
    "            vocab[char] = count\n",
    "            count += 1           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 19,\n",
       " '!': 65,\n",
       " '\"': 73,\n",
       " '$': 72,\n",
       " '&': 74,\n",
       " \"'\": 35,\n",
       " ',': 38,\n",
       " '-': 45,\n",
       " '.': 43,\n",
       " '/': 71,\n",
       " '0': 15,\n",
       " '1': 3,\n",
       " '2': 11,\n",
       " '3': 63,\n",
       " '4': 14,\n",
       " '5': 12,\n",
       " '6': 55,\n",
       " '7': 18,\n",
       " '8': 29,\n",
       " '9': 52,\n",
       " ':': 46,\n",
       " ';': 26,\n",
       " '?': 64,\n",
       " 'A': 36,\n",
       " 'B': 42,\n",
       " 'C': 25,\n",
       " 'D': 47,\n",
       " 'E': 49,\n",
       " 'F': 56,\n",
       " 'G': 34,\n",
       " 'H': 58,\n",
       " 'I': 59,\n",
       " 'J': 48,\n",
       " 'K': 61,\n",
       " 'L': 40,\n",
       " 'M': 51,\n",
       " 'N': 60,\n",
       " 'O': 54,\n",
       " 'P': 32,\n",
       " 'Q': 69,\n",
       " 'R': 50,\n",
       " 'S': 57,\n",
       " 'T': 41,\n",
       " 'U': 53,\n",
       " 'V': 62,\n",
       " 'W': 37,\n",
       " 'X': 70,\n",
       " 'Y': 44,\n",
       " 'Z': 67,\n",
       " 'a': 4,\n",
       " 'b': 17,\n",
       " 'c': 8,\n",
       " 'd': 22,\n",
       " 'e': 23,\n",
       " 'f': 2,\n",
       " 'g': 10,\n",
       " 'h': 24,\n",
       " 'i': 7,\n",
       " 'j': 33,\n",
       " 'k': 30,\n",
       " 'l': 20,\n",
       " 'm': 21,\n",
       " 'n': 5,\n",
       " 'o': 13,\n",
       " 'p': 9,\n",
       " 'q': 66,\n",
       " 'r': 0,\n",
       " 's': 6,\n",
       " 't': 1,\n",
       " 'u': 16,\n",
       " 'v': 27,\n",
       " 'w': 28,\n",
       " 'x': 31,\n",
       " 'y': 39,\n",
       " 'z': 68}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = ['<PAD>','<EOS>','<GO>']\n",
    "for code in codes:\n",
    "    vocab[code] = count\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  creating int_vo = {}  (in which according to occurance of word as a key character easily acess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now reversing the dictionary as \n",
    "int_vo ={}\n",
    "for i,j in vocab.items():\n",
    "    int_vo[j] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'r',\n",
       " 1: 't',\n",
       " 2: 'f',\n",
       " 3: '1',\n",
       " 4: 'a',\n",
       " 5: 'n',\n",
       " 6: 's',\n",
       " 7: 'i',\n",
       " 8: 'c',\n",
       " 9: 'p',\n",
       " 10: 'g',\n",
       " 11: '2',\n",
       " 12: '5',\n",
       " 13: 'o',\n",
       " 14: '4',\n",
       " 15: '0',\n",
       " 16: 'u',\n",
       " 17: 'b',\n",
       " 18: '7',\n",
       " 19: ' ',\n",
       " 20: 'l',\n",
       " 21: 'm',\n",
       " 22: 'd',\n",
       " 23: 'e',\n",
       " 24: 'h',\n",
       " 25: 'C',\n",
       " 26: ';',\n",
       " 27: 'v',\n",
       " 28: 'w',\n",
       " 29: '8',\n",
       " 30: 'k',\n",
       " 31: 'x',\n",
       " 32: 'P',\n",
       " 33: 'j',\n",
       " 34: 'G',\n",
       " 35: \"'\",\n",
       " 36: 'A',\n",
       " 37: 'W',\n",
       " 38: ',',\n",
       " 39: 'y',\n",
       " 40: 'L',\n",
       " 41: 'T',\n",
       " 42: 'B',\n",
       " 43: '.',\n",
       " 44: 'Y',\n",
       " 45: '-',\n",
       " 46: ':',\n",
       " 47: 'D',\n",
       " 48: 'J',\n",
       " 49: 'E',\n",
       " 50: 'R',\n",
       " 51: 'M',\n",
       " 52: '9',\n",
       " 53: 'U',\n",
       " 54: 'O',\n",
       " 55: '6',\n",
       " 56: 'F',\n",
       " 57: 'S',\n",
       " 58: 'H',\n",
       " 59: 'I',\n",
       " 60: 'N',\n",
       " 61: 'K',\n",
       " 62: 'V',\n",
       " 63: '3',\n",
       " 64: '?',\n",
       " 65: '!',\n",
       " 66: 'q',\n",
       " 67: 'Z',\n",
       " 68: 'z',\n",
       " 69: 'Q',\n",
       " 70: 'X',\n",
       " 71: '/',\n",
       " 72: '$',\n",
       " 73: '\"',\n",
       " 74: '&',\n",
       " 75: '<PAD>',\n",
       " 76: '<EOS>',\n",
       " 77: '<GO>'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_vo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting sentences from the bookclean\n",
    "sentences = []\n",
    "for i in book_clean:\n",
    "    for sen in i.split('.'):\n",
    "        sentences.append(sen + '.')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# converting all the sentence in integer by using the vocab dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting sentence into integer \n",
    "final_sentence = []\n",
    "for i in sentences:\n",
    "    b = []\n",
    "    for  char in i :\n",
    "        b.append(vocab[char])\n",
    "    final_sentence.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_sentence[0] [:5]\n",
    "# as in this assinging the values to the charcter in sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  spliting of dataset  into train, test by using train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100120\n",
      "33374\n"
     ]
    }
   ],
   "source": [
    "# now dividing the  whole data set into train and test by using train test split\n",
    "train , test =train_test_split(final_sentence , test_size = 0.25, random_state = 2)\n",
    "print(len(train))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4514\n"
     ]
    }
   ],
   "source": [
    "maxt = max([len(sentence) for sentence in train])\n",
    "print(maxt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   limiting the dataset length from 10 to 300 ( if len< 10 then not useful and len >300 contain much more complex and not useful to correct as mainly contain numerical value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting the sentence between len of 10 to 300 as make training faster \n",
    "train_sort = []\n",
    "min_length = 10 \n",
    "max_length = 300\n",
    "for i in range( min_length,max_length+2):\n",
    "    for j in train:\n",
    "        if (len(j) == i ):\n",
    "            train_sort.append(j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301\n"
     ]
    }
   ],
   "source": [
    "maxt = max([len(sentence) for sentence in train_sort])\n",
    "print(maxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we genrate noise in sentence \n",
    "letter =  ['a','b','c','d','e','f','g','h','i','j','k','l','m', 'n','o','p','q','r','s','t','u','v','w','x','y','z',]\n",
    "def noise(sentence, threshold):\n",
    "    noisy = []\n",
    "    i = 0\n",
    "    while i < len(sentence):\n",
    "        rand = np.random.uniform(0,0.9,1)\n",
    "        if rand < threshold:\n",
    "            noisy.append(sentence[i])\n",
    "        else:\n",
    "            new_rand= np.random.uniform(0,0.9,1)\n",
    "            if new_rand > 0.67:\n",
    "                if i == (len(sentence) - 1):\n",
    "                    continue\n",
    "                else:\n",
    "                    #noisy.append(sentence[i+1])\n",
    "                    #noisy.append(sentence[i])\n",
    "                    i += 1\n",
    "            elif new_rand < 0.33:\n",
    "                random_letter = np.random.choice(letter,1)[0]\n",
    "                noisy.insert(vocab[random_letter])\n",
    "                noisy.insert(sentence[i])\n",
    "            else:\n",
    "                pass     \n",
    "        i += 1\n",
    "    return noisy\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  create a noise function which make some random changes into train dataset soo that model can justify the changes and work accordingly and learining from this datset and grountruth "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "letter =  ['a','b','c','d','e','f','g','h','i','j','k','l','m', 'n','o','p','q','r','s','t','u','v','w','x','y','z',]\n",
    "def nois(sentence, threshold):\n",
    "    noisy = []\n",
    "    i = 0\n",
    "    while i < len(sentence):\n",
    "        rand = np.random.uniform(0,0.9,1)\n",
    "        if rand < threshold:\n",
    "            noisy.append(sentence[i])\n",
    "        else:\n",
    "            random_letter = np.random.choice(letter,1)[0]\n",
    "            a = vocab[random_letter]\n",
    "            c = len(sentence)\n",
    "            random_index = randint(0,c)\n",
    "            #sentence[i] = a\n",
    "            noisy.insert(random_index,a)\n",
    "            \n",
    "        i += 1\n",
    "    return noisy\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# example of the noise sentence create by noise function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19, 41, 24, 7, 6, 39, 6, 0, 6, 43]\n",
      "[19, 41, 24, 7, 6, 19, 51, 0, 6, 43]\n"
     ]
    }
   ],
   "source": [
    "f = nois(train_sort[0],0.8)\n",
    "print(f)\n",
    "print(train_sort[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n\n"
     ]
    }
   ],
   "source": [
    "ef =  int_vo[5]\n",
    "print(ef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# length of all  first 10 train_sort ( which contain all train data having length between 10 and 300 ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "88669\n"
     ]
    }
   ],
   "source": [
    "#b = []\n",
    "for i in train_sort[0:10]:\n",
    "    print(len(i))\n",
    "    #b.append(noise(i,0.9))\n",
    "print(len(train_sort))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_train = []\n",
    "b= 0.9\n",
    "for sentence in train_sort:\n",
    "    f = nois(sentence,b)\n",
    "    noisy_train.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noisy train = [] which contain noisy data that needed for the train with train_sort as the ground truth "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "for  i in noisy_train[:10]:\n",
    "    print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301\n",
      "[19, 41, 24, 7, 6, 19, 51, 0, 6, 43]\n",
      "[19, 41, 24, 7, 6, 19, 51, 0, 6, 43]\n"
     ]
    }
   ],
   "source": [
    "maxt = max([len(sentence) for sentence in noisy_train])\n",
    "print(maxt)\n",
    "print(noisy_train[0])\n",
    "print(train_sort[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  creating model in keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential \n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  padding   done in train_sort and noisy_train soo that all sentence have same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence(batch):\n",
    "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "    max_sentence = max([len(sentence) for sentence in batch])\n",
    "    return [sentence + [vocab['<PAD>']] * (max_sentence - len(sentence)) for sentence in batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_train= np.array(pad_sentence(train_sort))\n",
    "pad_noisy_train = np.array(pad_sentence(noisy_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(301,)\n",
      "(301,)\n"
     ]
    }
   ],
   "source": [
    "print(pad_train[2].shape)\n",
    "print(pad_noisy_train[2].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  model structure  of LSTM using keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, None, 40)          6720      \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, None, 20)          4880      \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, None, 1)           21        \n",
      "=================================================================\n",
      "Total params: 11,621\n",
      "Trainable params: 11,621\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(40, return_sequences=True, input_shape=(None, 1)))\n",
    "model.add(LSTM(20, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(1, activation='sigmoid')))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc']) \n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   in this basically normilization of pad_train , pad_noisy_train take place "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vibhu\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py:429: DataConversionWarning: Data with input dtype int32 was converted to float64 by the normalize function.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn  import preprocessing\n",
    "pd_train = preprocessing.normalize(pad_train)\n",
    "pd_noisy_train = preprocessing.normalize(pad_noisy_train)\n",
    "y = pd_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.01479943]\n",
      " [ 0.04439829]\n",
      " [ 0.01012593]\n",
      " [ 0.01479943]\n",
      " [ 0.04595612]\n",
      " [ 0.01479943]\n",
      " [ 0.00077892]\n",
      " [ 0.        ]\n",
      " [ 0.03037778]\n",
      " [ 0.03349344]]\n"
     ]
    }
   ],
   "source": [
    "a = pd_train.reshape(y[0],y[1],1)\n",
    "print(a[1][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.01479943]\n",
      " [ 0.04439829]\n",
      " [ 0.01012593]\n",
      " [ 0.01479943]\n",
      " [ 0.04595612]\n",
      " [ 0.01479943]\n",
      " [ 0.00077892]\n",
      " [ 0.        ]\n",
      " [ 0.03037778]\n",
      " [ 0.03349344]]\n"
     ]
    }
   ],
   "source": [
    "# similarly with noise data \n",
    "z = pd_noisy_train.shape\n",
    "b = pd_noisy_train.reshape(z[0],z[1],1)\n",
    "print(b[1][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  in this basically fitting the model  with a ( normalize train ground truth ) and b ( normalize train noisy  data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(a,b, batch_size=10, epochs=40, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  now similar whole procedure for testing data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  same procedure of preprocessing applied on testing data fiest preprocessing of data take place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sort = []\n",
    "min_length = 10 \n",
    "max_length = 300\n",
    "for i in range( min_length,max_length+2):\n",
    "    for j in train:\n",
    "        if (len(j) == i ):\n",
    "            test_sort.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_test = []\n",
    "b= 0.9\n",
    "for sentence in test_sort:\n",
    "    f = nois(sentence,b)\n",
    "    noisy_test.append(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# padding of the testing data take place as with pad_test and pad_noisy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_test= np.array(pad_sentence(test_sort))\n",
    "pad_noisy_test = np.array(pad_sentence(noisy_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  reshaping and normalization of testing data take place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_test = preprocessing.normalize(pad_test)\n",
    "pd_noisy_test = preprocessing.normalize(pad_noisy_test)\n",
    "y = pd_test.shape\n",
    "a1 = pd_test.reshape(y[0],y[1],1)\n",
    "z = pd_noisy_test.shape\n",
    "b1 = pd_noisy_test.reshape(z[0],z[1],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluation of model  take place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(a1, b1, verbose=0) \n",
    "print('Loss: %f, Accuracy: %f' % (loss, acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  getting and acurracy 79%  on test data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10): \n",
    "    yhat = model.predict_classes(X, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# at that  predicted sentence value and ground truth test sentence are shown for example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
